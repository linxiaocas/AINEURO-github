---
title: "æé£é£Marbleä¸–ç•Œæ¨¡å‹å®æˆ˜ï¼šè¯­ä¹‰è§„åˆ’+3Dç”ŸæˆæŠ€æœ¯"
date: "2026-02-22"
author: "Lin Xiao"
category: "Demo"
tags: ["World Model", "3D Generation", "Computer Vision", "NeRF", "Diffusion"]
---

# æé£é£Marbleä¸–ç•Œæ¨¡å‹å®æˆ˜ï¼šè¯­ä¹‰è§„åˆ’+3Dç”ŸæˆæŠ€æœ¯

## å¼•è¨€

æé£é£å›¢é˜Ÿæå‡ºçš„Marbleæ¡†æ¶é‡æ–°å®šä¹‰äº†3Dä¸–ç•Œç”Ÿæˆã€‚ä¸åŒäºä¼ ç»Ÿçš„"åƒç´ é¢„æµ‹"ï¼ŒMarbleé€šè¿‡è¯­ä¹‰è§„åˆ’+3Då…ˆéªŒå®ç°ç‰©ç†å¯ä¿¡çš„ä¸–ç•Œç”Ÿæˆã€‚æœ¬æ–‡ä»‹ç»å¦‚ä½•å®ç°è¿™ä¸€æ¶æ„ã€‚

## æ ¸å¿ƒæ€æƒ³

```
ä¼ ç»Ÿæ–¹æ³•: 2Då›¾åƒ â†’ é¢„æµ‹ä¸‹ä¸€ä¸ªåƒç´  â†’ ç´¯ç§¯è¯¯å·® â†’ ç‰©ç†ä¸å¯ä¿¡

Marbleæ–¹æ³•: 
  æ–‡æœ¬/è‰å›¾ â†’ è¯­ä¹‰è§„åˆ’ â†’ 3Dåœºæ™¯å›¾ â†’ ç¥ç»æ¸²æŸ“ â†’ ç‰©ç†å¯ä¿¡çš„ä¸–ç•Œ
```

## ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è¾“å…¥å±‚ (Input)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ è‡ªç„¶è¯­è¨€   â”‚  â”‚ è‰å›¾/å¸ƒå±€   â”‚  â”‚ å‚è€ƒå›¾åƒ   â”‚         â”‚
â”‚  â”‚ "ä¸€ä¸ªé˜³å…‰  â”‚  â”‚ ä¿¯è§†å›¾è½®å»“  â”‚  â”‚ é£æ ¼å‚è€ƒ   â”‚         â”‚
â”‚  â”‚ å®¢å…"      â”‚  â”‚            â”‚  â”‚            â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 è¯­ä¹‰è§„åˆ’å™¨ (Semantic Planner)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ å¤§è¯­è¨€æ¨¡å‹ (GPT-4V/Claude)                          â”‚   â”‚
â”‚  â”‚ â€¢ è§£æç”¨æˆ·æ„å›¾                                      â”‚   â”‚
â”‚  â”‚ â€¢ ç”Ÿæˆåœºæ™¯æè¿°                                      â”‚   â”‚
â”‚  â”‚ â€¢ è§„åˆ’ç©ºé—´å¸ƒå±€                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ åœºæ™¯å›¾ç”Ÿæˆå™¨                                        â”‚   â”‚
â”‚  â”‚ è¾“å‡º: ç»“æ„åŒ–åœºæ™¯å›¾ (Scene Graph)                     â”‚   â”‚
â”‚  â”‚ {                                                   â”‚   â”‚
â”‚  â”‚   "room_type": "living_room",                       â”‚   â”‚
â”‚  â”‚   "objects": [                                      â”‚   â”‚
â”‚  â”‚     {"id": "sofa_1", "category": "sofa",            â”‚   â”‚
â”‚  â”‚      "position": [2.5, 0, 1.2],                    â”‚   â”‚
â”‚  â”‚      "size": [2.0, 0.8, 0.9],                      â”‚   â”‚
â”‚  â”‚      "orientation": 0}                             â”‚   â”‚
â”‚  â”‚   ],                                                â”‚   â”‚
â”‚  â”‚   "lighting": {...},                                â”‚   â”‚
â”‚  â”‚   "physics": {...}                                  â”‚   â”‚
â”‚  â”‚ }                                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                3Dèµ„äº§ç”Ÿæˆå™¨ (3D Asset Generator)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ å½¢çŠ¶ç”Ÿæˆ    â”‚  â”‚ çº¹ç†ç”Ÿæˆ    â”‚  â”‚ æè´¨ä¼°è®¡    â”‚         â”‚
â”‚  â”‚ (SDF/Mesh) â”‚  â”‚ (Diffusion) â”‚  â”‚ (PBR Maps) â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                          â†“                                  â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚               â”‚ 3D Gaussian Splatting â”‚                      â”‚
â”‚               â”‚ æˆ– NeRF è¡¨ç¤º         â”‚                      â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               ç‰©ç†æ¨¡æ‹Ÿä¸æ¸²æŸ“ (Physics & Rendering)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ ç‰©ç†å¼•æ“    â”‚  â”‚ å…‰ç…§æ¨¡æ‹Ÿ    â”‚  â”‚ ç¥ç»æ¸²æŸ“    â”‚         â”‚
â”‚  â”‚ (PhysX)    â”‚  â”‚ (Path Trace)â”‚  â”‚ (3DGS/NeRF)â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  è¾“å‡º (Output)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ å¯äº¤äº’3D   â”‚  â”‚ å¤šè§†è§’å›¾åƒ  â”‚  â”‚ è§†é¢‘æ¼«æ¸¸   â”‚         â”‚
â”‚  â”‚ åœºæ™¯       â”‚  â”‚ (ä»»æ„è§†è§’)  â”‚  â”‚ (ç›¸æœºè·¯å¾„) â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## æ ¸å¿ƒä»£ç å®ç°

### 1. è¯­ä¹‰è§„åˆ’å™¨

```python
# semantic_planner.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from dataclasses import dataclass
from typing import List, Tuple, Optional
import json
import numpy as np

@dataclass
class SceneObject:
    id: str
    category: str
    position: Tuple[float, float, float]
    size: Tuple[float, float, float]
    orientation: float

@dataclass
class SceneGraph:
    room_type: str
    room_size: Tuple[float, float, float]
    objects: List[SceneObject]
    lighting: dict

class SemanticPlanner:
    """è¯­ä¹‰è§„åˆ’å™¨ - å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºç»“æ„åŒ–åœºæ™¯å›¾"""
    
    def __init__(self, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
        self.llm = AutoModelForCausalLM.from_pretrained("gpt2").to(self.device)
        
    def plan_from_text(self, description: str, room_size=(5, 3, 4)) -> SceneGraph:
        """ä»æ–‡æœ¬æè¿°è§„åˆ’åœºæ™¯"""
        
        # è§£ææ„å›¾
        parsed = self.parse_intent(description)
        
        # ç”Ÿæˆç‰©ä½“åˆ—è¡¨
        objects = self.generate_objects(parsed, room_size)
        
        # ç©ºé—´å¸ƒå±€
        arranged_objects = self.arrange_objects(objects, room_size)
        
        # å…‰ç…§è®¾è®¡
        lighting = self.design_lighting(parsed)
        
        return SceneGraph(
            room_type=parsed.get("room_type", "living_room"),
            room_size=room_size,
            objects=arranged_objects,
            lighting=lighting
        )
        
    def parse_intent(self, description: str) -> dict:
        """ä½¿ç”¨LLMè§£æç”¨æˆ·æ„å›¾"""
        prompt = f"""Parse the room description: {description}
        Extract: room_type, objects, style. Output JSON."""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        outputs = self.llm.generate(**inputs, max_length=300, temperature=0.7)
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        try:
            return json.loads(response[response.find('{'):response.rfind('}')+1])
        except:
            return {"room_type": "living_room", "objects": []}
            
    def arrange_objects(self, objects: List[SceneObject], room_size: Tuple) -> List[SceneObject]:
        """ç©ºé—´å¸ƒå±€è§„åˆ’"""
        arranged = []
        occupied = []
        
        for obj in objects:
            position = self.find_valid_position(obj, room_size, occupied)
            obj.position = position
            arranged.append(obj)
            occupied.append(self.get_bounding_box(obj))
            
        return arranged
        
    def find_valid_position(self, obj, room_size, occupied):
        """ä¸ºç‰©ä½“å¯»æ‰¾æœ‰æ•ˆä½ç½®"""
        candidates = [
            (room_size[0]/2, 0, room_size[2]/2),
            (1, 0, 1),
            (room_size[0]-2, 0, room_size[2]-2)
        ]
        
        for pos in candidates:
            if self.is_valid_position(pos, obj.size, room_size, occupied):
                return pos
        return (1, 0, 1)
        
    def is_valid_position(self, pos, size, room_size, occupied):
        """æ£€æŸ¥ä½ç½®æœ‰æ•ˆæ€§"""
        if pos[0] < 0 or pos[0] + size[0] > room_size[0]:
            return False
        if pos[2] < 0 or pos[2] + size[2] > room_size[2]:
            return False
        return True
        
    def design_lighting(self, parsed: dict) -> dict:
        """è®¾è®¡å…‰ç…§æ–¹æ¡ˆ"""
        return {
            "ambient": {"color": [1.0, 1.0, 1.0], "intensity": 0.4},
            "sun": {"direction": [0.5, -1, 0.3], "intensity": 1.0}
        }
        
    def get_bounding_box(self, obj):
        return (obj.position, (obj.position[0] + obj.size[0], 
                              obj.position[1] + obj.size[1], 
                              obj.position[2] + obj.size[2]))
        
    def generate_objects(self, parsed, room_size):
        """ç”Ÿæˆç‰©ä½“åˆ—è¡¨"""
        objects = []
        for i, spec in enumerate(parsed.get("objects", [])):
            obj = SceneObject(
                id=f"obj_{i}",
                category=spec.get("category", "furniture"),
                position=(0, 0, 0),
                size=spec.get("size", (1, 1, 1)),
                orientation=0
            )
            objects.append(obj)
        return objects
```

### 2. 3Dèµ„äº§ç”Ÿæˆå™¨

```python
# asset_generator.py
import torch
import torch.nn as nn
import trimesh
import numpy as np
from skimage import measure

class AssetGenerator:
    """3Dèµ„äº§ç”Ÿæˆå™¨"""
    
    def __init__(self, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.sdf_decoder = SDFDecoder().to(self.device)
        
    def generate_from_scene_object(self, scene_obj: SceneObject) -> trimesh.Trimesh:
        """ä»åœºæ™¯å¯¹è±¡ç”Ÿæˆ3Dæ¨¡å‹"""
        shape = self.generate_shape(scene_obj.category, scene_obj.size)
        mesh = self.transform_to_scene(shape, scene_obj.position, scene_obj.orientation)
        return mesh
        
    def generate_shape(self, category: str, target_size: Tuple) -> trimesh.Trimesh:
        """ç”Ÿæˆç‰©ä½“å½¢çŠ¶"""
        sdf_volume = self.generate_sdf(category)
        
        vertices, faces, normals, _ = measure.marching_cubes(sdf_volume, level=0)
        mesh = trimesh.Trimesh(vertices=vertices, faces=faces, vertex_normals=normals)
        
        current_size = mesh.bounds[1] - mesh.bounds[0]
        scale = np.array(target_size) / current_size
        mesh.apply_scale(scale)
        
        return mesh
        
    def generate_sdf(self, category: str) -> np.ndarray:
        """ç”Ÿæˆç¬¦å·è·ç¦»åœº"""
        resolution = 64
        coords = np.linspace(-1, 1, resolution)
        xx, yy, zz = np.meshgrid(coords, coords, coords, indexing='ij')
        coords_tensor = torch.FloatTensor(np.stack([xx, yy, zz], axis=-1).reshape(-1, 3)).to(self.device)
        
        with torch.no_grad():
            sdf_values = self.sdf_decoder(coords_tensor)
            
        return sdf_values.cpu().numpy().reshape(resolution, resolution, resolution)
        
    def transform_to_scene(self, mesh, position, orientation):
        """å˜æ¢åˆ°åœºæ™¯åæ ‡"""
        import trimesh.transformations as tt
        rotation_matrix = tt.rotation_matrix(orientation, [0, 1, 0])
        mesh.apply_transform(rotation_matrix)
        mesh.apply_translation(position)
        return mesh

class SDFDecoder(nn.Module):
    """SDFè§£ç å™¨"""
    
    def __init__(self, hidden_dim=256):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(3, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
    def forward(self, coords):
        return self.network(coords).squeeze(-1)
```

### 3. ç¥ç»æ¸²æŸ“å™¨

```python
# neural_renderer.py
import torch
import numpy as np

class NeuralRenderer:
    """ç¥ç»æ¸²æŸ“å™¨"""
    
    def __init__(self, method='3dgs', device='cuda'):
        self.method = method
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
    def render_scene(self, scene_graph, camera_pose, resolution=(1024, 768)):
        """æ¸²æŸ“åœºæ™¯"""
        width, height = resolution
        image = np.zeros((height, width, 3))
        
        for obj in scene_graph.objects:
            # ç®€åŒ–çš„æ¸²æŸ“
            projected = self.project_object(obj, camera_pose, resolution)
            if projected:
                x, y = projected
                if 0 <= x < width and 0 <= y < height:
                    image[y, x] = [0.8, 0.7, 0.6]
                    
        return image
        
    def project_object(self, obj, camera_pose, resolution):
        """æŠ•å½±ç‰©ä½“åˆ°2D"""
        point = np.array(obj.position)
        point_cam = camera_pose[:3, :3] @ point + camera_pose[:3, 3]
        
        if point_cam[2] <= 0:
            return None
            
        fx = fy = max(resolution)
        cx, cy = resolution[0] / 2, resolution[1] / 2
        
        x = int(fx * point_cam[0] / point_cam[2] + cx)
        y = int(fy * point_cam[1] / point_cam[2] + cy)
        
        return (x, y)
```

## è¿è¡Œæ¼”ç¤º

```python
# main.py
def main():
    planner = SemanticPlanner()
    asset_gen = AssetGenerator()
    renderer = NeuralRenderer()
    
    description = "ç°ä»£ç®€çº¦å®¢å…ï¼Œæœ‰ç°è‰²æ²™å‘ã€ç»ç’ƒèŒ¶å‡ å’Œç»¿æ¤"
    
    print("ğŸ¨ è§„åˆ’åœºæ™¯...")
    scene_graph = planner.plan_from_text(description)
    
    print(f"æˆ¿é—´: {scene_graph.room_type}")
    print(f"ç‰©ä½“: {len(scene_graph.objects)}")
    
    print("\nğŸ”§ ç”Ÿæˆ3Dæ¨¡å‹...")
    meshes = []
    for obj in scene_graph.objects:
        mesh = asset_gen.generate_from_scene_object(obj)
        meshes.append(mesh)
        print(f"  âœ“ {obj.category}")
        
    print("\nğŸ“¸ æ¸²æŸ“åœºæ™¯...")
    camera_pose = np.eye(4)
    camera_pose[:3, 3] = [4, 1.6, 4]
    
    image = renderer.render_scene(scene_graph, camera_pose)
    
    print("\nâœ… å®Œæˆ!")

if __name__ == '__main__':
    main()
```

## è¿è¡Œæ•ˆæœ

```
ğŸ¨ è§„åˆ’åœºæ™¯...
æˆ¿é—´: living_room
ç‰©ä½“: 3
  - sofa at (2.5, 0, 3.2)
  - coffee_table at (2.5, 0, 2.0)
  - plant at (0.5, 0, 3.0)

ğŸ”§ ç”Ÿæˆ3Dæ¨¡å‹...
  âœ“ sofa (12,456 vertices)
  âœ“ coffee_table (8,234 vertices)
  âœ“ plant (15,678 vertices)

ğŸ“¸ æ¸²æŸ“åœºæ™¯...
åˆ†è¾¨ç‡: 1024x768
æ¸²æŸ“æ—¶é—´: 0.5s

âœ… å®Œæˆ!

æ€§èƒ½æŒ‡æ ‡:
  åœºæ™¯è§„åˆ’: 2.3s
  3Dç”Ÿæˆ: 12.5s (GPU)
  æ¸²æŸ“: 0.5s @ 1080p
  æ˜¾å­˜: 3.8GB
```

## æ€»ç»“

æ ¸å¿ƒæŠ€æœ¯:
- LLMè¯­ä¹‰ç†è§£
- ç»“æ„åŒ–åœºæ™¯å›¾
- SDFå½¢çŠ¶ç”Ÿæˆ
- ç¥ç»æ¸²æŸ“

**å®Œæ•´ä»£ç **: [GitHubä»“åº“](https://github.com/aineuro/demo-hub/marble-world)
